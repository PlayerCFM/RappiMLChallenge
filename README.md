
![Rappi Logo](https://d1yjjnpx0p53s8.cloudfront.net/styles/logo-thumbnail/s3/072017/untitled-1_98.png?itok=9OSsQQAz)

# Rappi's Challenge: Titanic Dataset

  

The following repository includes my solution for the Machine Learning ML Engineer Challenge.

The project follows an object-oriented programming approach. There are interfaces to support multiple datasets and machine learning models.

The interface for multiple datasets support was developed since each dataset has a different set of preprocessing steps.
In addition, the interface to support multiple models helps to inherit from such interface and write machine learning models using different libraries such as Pytorch, SciKit-Learn and more, in a generalized way.

Inheriting from these interfaces and implementing their abstract methods helps to generalize and run the training pipeline quickly.

In addition, there are enumerators that list the different types of supported scaling methods, data and models in the project.
  

# Project Structure

The project is structured in the following way:

  

Rappi/

├── Core: Major project/machine learning steps.

├── Data: All .csv files related to train and test data.

├── DataLoader: Classes for loading and managing the datasets.

├── Models/: Classes for supported models, and binary files for model's and scaler's weights.

│ ├── SavedModels

│ └── SavedScalers

├── Tests: Project unit tests.

├── Utils: Utility used along the project.

├── Main.ipynb: Visual representation of the code by Jupyter notebook. Also testing and deployment explanation.

├── Main.py: Used for CLI functionality.

└── README.md: This file.

  

# CLI

The first part includes a Command Line Interface for the project.

This CLI allows to run the project in a terminal, able to run a variety of operations like:

* List and show the supported data.

* List and load pre-trained models.

* Run a complete training pipeline.

* Show model's performance.

  

# Jupyter Notebook

The Jupyter notebook uses the code to show visually how the code works, as well as code explanation, feature importance and model justification.

This notebook also explains my approach to model deployment using AWS and unit test explanation.

  

# How To Run

To have a better dependency management, isolation and portability; this project was developed using the Conda Virtual Environment Manager.

That's why you have to have a Conda installation before using this project.

  

Once you have it, you can install the dependencies in the `environment.yml` file by the following command:

  

    conda env create -f environment.yml

  

This command will create a new virtual environment called `rappichallenge` and install the project dependencies on it.

  

Run the project CLI by typing:

    python Main.py

This command will start the CLI and you can start choosing the different options to perform actions like, model loading, training, inference, and more...

  

On the other hand, you can use the Jupyter Notebook by using your favorite Jupyter server (in my case, I used the VS Code extesion for Notebook development).

  

The Jupyter notebook offers a much more visual way to walk through the code, looking at different outputs and graphs.

  

In addition, justifications and answers to questions are provided in this notebook:

Is the model a good model to use for inference?

How to put the model into a production environment?

What do the Coverage results and the unit tests mean?

  

# Test Report

The test report was generated by using the Coverage library.

This report indicates the effectiveness of the unit test written.

This coverage report indicates which parts of the code are executed by the testing and with others are not.

  

You can see Coverage report results in the Jupyter Notebook in a table form

Such table presents all the files that are part of the project and how the unit test process covered them.

  

The table is comprised of the following columns:

Module: Name of the file.

Statements: Number of statements part of the coverage execution.

Covered Lines: Number of lines that were executed out of the total lines part of the coverage.

Missing: How many lines were not executed from the total statements in the coverage run.

Coverage: Coverage of lines executed in percentage.

Excluded: Lines not part of the Coverage execution.

  

With coverage reports, we aim to get close to 100% coverage, because that way we can be sure that most of our code is properly tested before it goes into production.

  

However, there is some debate in the community about achieving such a percentage because it can be time-consuming, expensive, and otherwise inconvenient.

  

While code coverage is important to ensure the effectiveness of unit tests, it is not always necessary or practical to aim for 100% coverage. Focusing on writing high-quality tests that cover critical functionality, and prioritizing testing efforts based on the specific needs of the project, is more important.

  

# Deployment


How can we put into production a model that we are satisfied with according to the metrics?

Since we are storing the weights of the models in a folder, we can read them directly from the directory where they are located and put them into production.

The most common way to do this is to expose them through an API.

  

In Python there are several libraries that allow us to create a web server with very few lines of code. One of them is FastAPI. Combining FastAPI with Docker offers us a very powerful way of containerization that will help us to lift an API from our model and it can start being consumed in a matter of minutes.

  

My approach to send a model to Production would be the following:

  

1. Build a FastAPI based project with POST requests so that the model can make inferences.

2. The FastAPI code should read the selected model from the specified directory, and then load it.

3. Generate a Dockerfile (_that you can find in the root directory of this project_) containing different steps such as:

* Using a Python based base image.

* Install the project requirements.

* Copy the FastAPI-based project files to the directory, for example, /app of the container.

* install the requirements from the requirements.txt file

* Expose the container port

* Run the FastAPI project

4. Use a Virtual Machine from an On-Premise or a Cloud environment to host our code.

  

`*This is a general pipeline*. Other aspects should also be covered like`:

* Store Logging information.

* Monitor the performance of the model over time.

* Scaling to support of houndreds of users.

* Addind security measures.

* And more...

  

## Cloud Architecture

For deploying this machine learning model, I would suggest to select an architecture based on the following question: "Do we need to inference data in real time?"

If the answer is Yes, then we should consider use a real-time (or "online) archirecture: This type of architecture allow use to receive a query and return an inference as it is received with a very low latency between client and server. However, this is a very expensive to implement.

If the answer it No, we can use a batch processing architecture where the model processed a batch of data instead of just a single record to then make an inference on that batch of data. This architecture should be used when the reponse time is not important and working with a large volume of data.

  
![AWS Logo](https://ltre-web.azureedge.net/1986/aws-logo.png)

I also suggets to use a more automated pipeline for deployment, by using AWS:

* Data storage: Store the training and testing data in a Amazon S3 Bucket.

* Data preprocessing: Create an AWS Lambda for pre-processing our dataset. Lambda is recommended over a continuously running python script because the preprocessing step for training a model runs just once.

* Model training: Use Amazon SageMaker to create, train and deploy the model into production.

* Store model weights back to the S3 Bucket.

* Model evaluation: Use Amazon SageMaker as well.

  

* Note; Alternatively, we can train the model using a EC3 instance and expose a web server.

  

Amazon Web Services also helps with load balacing and by using a Kubernetes serice to manage all the containers.